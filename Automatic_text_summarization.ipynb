{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarkiAnuj17/Advanced-Calculator/blob/main/Automatic_text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUshmJtq0hCv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EtxIWDza0kMp"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/dataset.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/dataset\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qo35kKPv0kRP"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cudf\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk import pos_tag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_oX8kZ610kT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5827181-1f02-443f-a6f8-d8423d40dcd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'article', 'highlights'], dtype='object')\n",
            "(11490, 3)\n"
          ]
        }
      ],
      "source": [
        "path=\"/content/dataset/dataset/test.csv\"\n",
        "df=pd.read_csv(path)\n",
        "print(df.columns)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wufX14cj0kWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5197aa1c-dc39-40d9-e49a-933bfa06f2aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6lN5tja0kYz",
        "outputId": "d55c2302-fbb5-4f1d-bec3-a44132a95ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11490/11490 [09:07<00:00, 20.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         id  \\\n",
            "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
            "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
            "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
            "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
            "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
            "\n",
            "                                             article  \\\n",
            "0  Ever noticed how plane seats appear to be gett...   \n",
            "1  A drunk teenage boy had to be rescued by secur...   \n",
            "2  Dougie Freedman is on the verge of agreeing a ...   \n",
            "3  Liverpool target Neto is also wanted by PSG an...   \n",
            "4  Bruce Jenner will break his silence in a two-h...   \n",
            "\n",
            "                                          highlights  \\\n",
            "0  Experts question if  packed out planes are put...   \n",
            "1  Drunk teenage boy climbed into lion enclosure ...   \n",
            "2  Nottingham Forest are close to extending Dougi...   \n",
            "3  Fiorentina goalkeeper Neto has been linked wit...   \n",
            "4  Tell-all interview with the reality TV star, 6...   \n",
            "\n",
            "                                      processed_text  \n",
            "0  [[ever, notice, plane, seat, appear, get, smal...  \n",
            "1  [[drunk, teenage, boy, rescue, security, jump,...  \n",
            "2  [[dougie, freedman, verge, agree, new, deal, r...  \n",
            "3  [[liverpool, target, neto, also, want, psg, cl...  \n",
            "4  [[bruce, jenner, break, silence, interview, di...  \n"
          ]
        }
      ],
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    # Map POS tag to WordNet POS tag\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Split text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Get English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Preprocess each sentence\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Tokenize words in the sentence\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        # Get POS tags\n",
        "        pos_tags = pos_tag(words)\n",
        "\n",
        "        # Lemmatize and remove stopwords\n",
        "        processed_words = []\n",
        "        for word, pos in pos_tags:\n",
        "            if word.lower() not in stop_words and word.isalnum():\n",
        "                lemmatized_word = lemmatizer.lemmatize(word.lower(), get_wordnet_pos(pos))\n",
        "                processed_words.append(lemmatized_word)\n",
        "\n",
        "        # Append processed sentence\n",
        "        processed_sentences.append(processed_words)\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "def preprocess_csv(file_path, text_column):\n",
        "    # Read the CSV file into a cuDF DataFrame\n",
        "    df = cudf.read_csv(file_path)\n",
        "\n",
        "    # Convert text column to a pandas Series for processing with NLTK\n",
        "    text_series = df[text_column].to_pandas()\n",
        "\n",
        "    # Apply preprocessing to the text column with progress bar\n",
        "    processed_text = text_series.progress_apply(preprocess_text)\n",
        "\n",
        "    # Convert the processed text back to a cuDF DataFrame\n",
        "    df['processed_text'] = cudf.from_pandas(processed_text)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Enable the tqdm progress_apply\n",
        "tqdm.pandas()\n",
        "\n",
        "file_path = '/content/dataset/dataset/test.csv'\n",
        "text_column = 'article'\n",
        "processed_df = preprocess_csv(file_path, text_column)\n",
        "\n",
        "# Display the processed DataFrame\n",
        "print(processed_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nGGkaA3FffZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0dbe64f-dcb0-4a88-c803-0ad77da152d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Articles: 100%|██████████| 11490/11490 [01:26<00:00, 133.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Cosine Similarity Matrix for the First Article:\n",
            " [[1.         0.09128709 0.         0.1        0.06454972 0.\n",
            "  0.         0.16329932 0.09534626 0.0766965  0.08451543 0.16903085\n",
            "  0.21764288 0.         0.048795   0.11952286]\n",
            " [0.09128709 1.         0.09622504 0.18257419 0.05892557 0.\n",
            "  0.20412415 0.0745356  0.08703883 0.         0.15430335 0.07715167\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.09622504 1.         0.63245553 0.13608276 0.19245009\n",
            "  0.         0.0860663  0.         0.         0.         0.\n",
            "  0.         0.         0.05143445 0.        ]\n",
            " [0.1        0.18257419 0.63245553 1.         0.12909944 0.09128709\n",
            "  0.         0.16329932 0.09534626 0.         0.08451543 0.08451543\n",
            "  0.         0.         0.048795   0.        ]\n",
            " [0.06454972 0.05892557 0.13608276 0.12909944 1.         0.29462783\n",
            "  0.         0.10540926 0.12309149 0.         0.10910895 0.10910895\n",
            "  0.         0.         0.03149704 0.        ]\n",
            " [0.         0.         0.19245009 0.09128709 0.29462783 1.\n",
            "  0.         0.0745356  0.         0.         0.07715167 0.\n",
            "  0.         0.         0.04454354 0.        ]\n",
            " [0.         0.20412415 0.         0.         0.         0.\n",
            "  1.         0.         0.10660036 0.         0.09449112 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.16329932 0.0745356  0.0860663  0.16329932 0.10540926 0.0745356\n",
            "  0.         1.         0.07784989 0.06262243 0.06900656 0.13801311\n",
            "  0.17770466 0.         0.07968191 0.09759001]\n",
            " [0.09534626 0.08703883 0.         0.09534626 0.12309149 0.\n",
            "  0.10660036 0.07784989 1.         0.29250897 0.24174689 0.72524067\n",
            "  0.06917145 0.49236596 0.27914526 0.34188173]\n",
            " [0.0766965  0.         0.         0.         0.         0.\n",
            "  0.         0.06262243 0.29250897 1.         0.         0.32410186\n",
            "  0.16692447 0.39605902 0.71105713 0.41251432]\n",
            " [0.08451543 0.15430335 0.         0.08451543 0.10910895 0.07715167\n",
            "  0.09449112 0.06900656 0.24174689 0.         1.         0.21428571\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.16903085 0.07715167 0.         0.08451543 0.10910895 0.\n",
            "  0.         0.13801311 0.72524067 0.32410186 0.21428571 1.\n",
            "  0.1839418  0.32732684 0.28867513 0.30304576]\n",
            " [0.21764288 0.         0.         0.         0.         0.\n",
            "  0.         0.17770466 0.06917145 0.16692447 0.         0.1839418\n",
            "  1.         0.09365858 0.10619885 0.34684399]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.49236596 0.39605902 0.         0.32732684\n",
            "  0.09365858 1.         0.37796447 0.46291005]\n",
            " [0.048795   0.         0.05143445 0.048795   0.03149704 0.04454354\n",
            "  0.         0.07968191 0.27914526 0.71105713 0.         0.28867513\n",
            "  0.10619885 0.37796447 1.         0.49573007]\n",
            " [0.11952286 0.         0.         0.         0.         0.\n",
            "  0.         0.09759001 0.34188173 0.41251432 0.         0.30304576\n",
            "  0.34684399 0.46291005 0.49573007 1.        ]]\n"
          ]
        }
      ],
      "source": [
        "import cupy as cp\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Vectorize sentences using a Bag-of-Words approach with CuPy\n",
        "def vectorize_sentences(sentences):\n",
        "    # Create vocabulary (unique words across all sentences)\n",
        "    vocabulary = sorted(set(word for sentence in sentences for word in sentence))\n",
        "    word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "    # Create term-document matrix using CuPy\n",
        "    term_doc_matrix = cp.zeros((len(sentences), len(word_to_index)))\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        word_counts = Counter(sentence)\n",
        "        for word, count in word_counts.items():\n",
        "            if word in word_to_index:\n",
        "                term_doc_matrix[i, word_to_index[word]] = count\n",
        "    return term_doc_matrix\n",
        "\n",
        "# Compute the cosine similarity matrix using CuPy\n",
        "def cosine_similarity_matrix(matrix):\n",
        "    # Matrix should be a CuPy array\n",
        "    matrix = cp.array(matrix)\n",
        "    dot_product = cp.dot(matrix, matrix.T)\n",
        "    norm = cp.linalg.norm(matrix, axis=1)\n",
        "    similarity_matrix = dot_product / (cp.outer(norm, norm) + 1e-10)  # Prevent division by zero\n",
        "    return similarity_matrix\n",
        "\n",
        "# Process each article and compute sentence similarity matrices\n",
        "def compute_similarity_for_articles(df, text_column='processed_text'):\n",
        "    text_series = df[text_column].to_pandas()\n",
        "\n",
        "    # List to hold similarity matrices for each article\n",
        "    similarity_matrices = []\n",
        "\n",
        "    # Iterate over each article\n",
        "    for index, processed_text in tqdm(text_series.items(), total=len(text_series), desc=\"Processing Articles\"):\n",
        "        processed_sentences = [' '.join(sentence) for sentence in processed_text]\n",
        "\n",
        "        # Vectorize sentences\n",
        "        term_doc_matrix = vectorize_sentences(processed_text)\n",
        "\n",
        "        # Compute similarity matrix\n",
        "        similarity_matrix = cosine_similarity_matrix(term_doc_matrix)\n",
        "\n",
        "        # Convert to CPU if needed for output (optional if needed)\n",
        "        similarity_matrix_cpu = cp.asnumpy(similarity_matrix)\n",
        "\n",
        "        # Store the similarity matrix for the article\n",
        "        similarity_matrices.append(similarity_matrix_cpu)\n",
        "\n",
        "    return similarity_matrices\n",
        "\n",
        "# Compute and store similarity matrices for each article\n",
        "final_similarity_matrices = compute_similarity_for_articles(processed_df, text_column='processed_text')\n",
        "\n",
        "# Example output for the first article\n",
        "print(\"Final Cosine Similarity Matrix for the First Article:\\n\", final_similarity_matrices[0])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}